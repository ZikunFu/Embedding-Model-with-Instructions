{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching cs.* articles: 100%|██████████| 2500/2500 [01:50<00:00, 22.57it/s]\n",
      "Fetching econ.* articles: 100%|██████████| 2500/2500 [01:31<00:00, 27.29it/s]\n",
      "Fetching eess.* articles: 100%|██████████| 2500/2500 [01:30<00:00, 27.62it/s]\n",
      "Fetching math.* articles:  72%|███████▏  | 1800/2500 [01:57<00:45, 15.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching math.* articles. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching math.* articles:  68%|██████▊   | 1701/2500 [01:50<01:03, 12.64it/s]Bozo feed; consider handling: document declared as utf-8, but parsed as windows-1252\n",
      "Fetching math.* articles:  80%|████████  | 2000/2500 [02:30<00:37, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching math.* articles. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching math.* articles:  68%|██████▊   | 1701/2500 [01:51<00:52, 15.26it/s]Bozo feed; consider handling: document declared as utf-8, but parsed as windows-1252\n",
      "Fetching math.* articles:  76%|███████▌  | 1900/2500 [02:10<00:41, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching math.* articles. Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching physics.* articles: 100%|██████████| 2500/2500 [02:05<00:00, 19.93it/s]\n",
      "Fetching q-bio.* articles: 100%|██████████| 2500/2500 [01:39<00:00, 25.06it/s]\n",
      "Fetching q-fin.* articles: 100%|██████████| 2500/2500 [01:34<00:00, 26.33it/s]\n",
      "Fetching stat.* articles: 100%|██████████| 2500/2500 [01:45<00:00, 23.65it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f59edef7a4460c9a86f497fee64c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340351212d6f483babaee0e0e8665ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import arxiv\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Function to fetch articles from arXiv for a specific category\n",
    "def fetch_articles(category, max_results=1250, retries=3):\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=f\"cat:{category}\",\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    results = []\n",
    "    for result in tqdm(client.results(search), total=max_results, desc=f\"Fetching {category} articles\"):\n",
    "        results.append({\"abstract\": result.summary, \"category\": category})\n",
    "    return results\n",
    "\n",
    "# Define primary categories with arXiv prefixes\n",
    "categories = [\n",
    "    \"cs.*\",       # Computer Science\n",
    "    \"econ.*\",     # Economics\n",
    "    \"eess.*\",     # Electrical Engineering and Systems Science\n",
    "    \"math.*\",     # Mathematics\n",
    "    \"physics.*\",  # Physics\n",
    "    \"q-bio.*\",    # Quantitative Biology\n",
    "    \"q-fin.*\",    # Quantitative Finance\n",
    "    \"stat.*\"      # Statistics\n",
    "]\n",
    "\n",
    "# Fetch articles for each category and split into train and test\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for category in categories:\n",
    "    attempts = 0\n",
    "    success = False\n",
    "    while attempts < 3 and not success:\n",
    "        try:\n",
    "            articles = fetch_articles(category, 2500)  # Fetch double the amount to split into train/test\n",
    "            train_data.extend(articles[:1250])\n",
    "            test_data.extend(articles[1250:2500])\n",
    "            success = True\n",
    "        except arxiv.UnexpectedEmptyPageError:\n",
    "            print(f\"Error fetching {category} articles. Retrying...\")\n",
    "            attempts += 1\n",
    "            time.sleep(5)  # Wait for a few seconds before retrying\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Save the dataset in JSON format\n",
    "dataset_dict.save_to_disk(\"arxiv_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching math.* articles:  65%|██████▌   | 1701/2600 [01:33<00:41, 21.68it/s]Bozo feed; consider handling: document declared as utf-8, but parsed as windows-1252\n",
      "Fetching math.* articles:  96%|█████████▌| 2499/2600 [02:03<00:04, 20.32it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1203054db6e144aca61f4338df5f1671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46af94f0d0e8497e9a81e2ae407d00ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved successfully!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['abstract', 'category'],\n",
      "        num_rows: 8750\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['abstract', 'category'],\n",
      "        num_rows: 8750\n",
      "    })\n",
      "})\n",
      "Sample train data: {'abstract': 'The goal of this paper is to define and analyze systems which exhibit brittle\\nbehavior. This behavior is characterized by a sudden and steep decline in\\nperformance as the system approaches the limits of tolerance. This can be due\\nto input parameters which exceed a specified input, or environmental conditions\\nwhich exceed specified operating boundaries. An analogy is made between brittle\\ncommmunication systems in particular and materials science.', 'category': 'cs.*'}\n",
      "Sample test data: {'abstract': 'Sample-efficient machine learning (SEML) has been widely applied to find\\noptimal latency and power tradeoffs for configurable computer systems. Instead\\nof randomly sampling from the configuration space, SEML reduces the search cost\\nby dramatically reducing the number of configurations that must be sampled to\\noptimize system goals (e.g., low latency or energy). Nevertheless, SEML only\\nreduces one component of cost -- the total number of samples collected -- but\\ndoes not decrease the cost of collecting each sample. Critically, not all\\nsamples are equal; some take much longer to collect because they correspond to\\nslow system configurations. This paper present Cello, a computer systems\\noptimization framework that reduces sample collection costs -- especially those\\nthat come from the slowest configurations. The key insight is to predict ahead\\nof time whether samples will have poor system behavior (e.g., long latency or\\nhigh energy) and terminate these samples early before their measured system\\nbehavior surpasses the termination threshold, which we call it predictive early\\ntermination. To predict the future system behavior accurately before it\\nmanifests as high runtime or energy, Cello uses censored regression to produces\\naccurate predictions for running samples. We evaluate Cello by optimizing\\nlatency and energy for Apache Spark workloads. We give Cello a fixed amount of\\ntime to search a combined space of hardware and software configuration\\nparameters. Our evaluation shows that compared to the state-of-the-art SEML\\napproach in computer systems optimization, Cello improves latency by 1.19X for\\nminimizing latency under a power constraint, and improves energy by 1.18X for\\nminimizing energy under a latency constraint.', 'category': 'cs.*'}\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from datasets import Dataset, DatasetDict, load_from_disk, concatenate_datasets\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Function to fetch articles from arXiv for a specific category with extra papers to handle errors\n",
    "def fetch_articles(category, max_results=1250, extra=100):\n",
    "    client = arxiv.Client()\n",
    "    total_results = max_results + extra\n",
    "    search = arxiv.Search(\n",
    "        query=f\"cat:{category}\",\n",
    "        max_results=total_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    results = []\n",
    "    for result in tqdm(client.results(search), total=total_results, desc=f\"Fetching {category} articles\"):\n",
    "        try:\n",
    "            results.append({\"abstract\": result.summary, \"category\": category})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article: {e}\")\n",
    "        if len(results) >= max_results:\n",
    "            break\n",
    "    return results[:max_results]  # Ensure we only return the desired number of results\n",
    "\n",
    "# Define the math category with arXiv prefix\n",
    "math_category = \"math.*\"\n",
    "\n",
    "# Fetch math articles\n",
    "math_articles = fetch_articles(math_category, 2500)\n",
    "\n",
    "# Split into train and test\n",
    "math_train_data = math_articles[:1250]\n",
    "math_test_data = math_articles[1250:2500]\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "math_train_df = pd.DataFrame(math_train_data)\n",
    "math_test_df = pd.DataFrame(math_test_data)\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "math_train_dataset = Dataset.from_pandas(math_train_df)\n",
    "math_test_dataset = Dataset.from_pandas(math_test_df)\n",
    "\n",
    "# Load the existing dataset\n",
    "dataset_dict = load_from_disk(\"arxiv_dataset\")\n",
    "\n",
    "# Append the new data to the existing dataset\n",
    "train_dataset = concatenate_datasets([dataset_dict['train'], math_train_dataset])\n",
    "test_dataset = concatenate_datasets([dataset_dict['test'], math_test_dataset])\n",
    "\n",
    "updated_dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Save the updated dataset\n",
    "dataset_dict.save_to_disk(\"updated_arxiv_dataset\")\n",
    "\n",
    "print(\"Updated dataset saved successfully!\")\n",
    "\n",
    "# Load the updated dataset back to test if it works\n",
    "loaded_dataset = load_from_disk(\"updated_arxiv_dataset\")\n",
    "print(loaded_dataset)\n",
    "\n",
    "# Print some sample data to verify\n",
    "print(\"Sample train data:\", loaded_dataset['train'][0])\n",
    "print(\"Sample test data:\", loaded_dataset['test'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0414cbf18964e4e834f4d323e8a8c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5281f4065545c1a70c6586ef33f805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "updated_dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# Save the updated dataset\n",
    "updated_dataset_dict.save_to_disk(\"updated_arxiv_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "Sample train data: {'text': 'The goal of this paper is to define and analyze systems which exhibit brittle\\nbehavior. This behavior is characterized by a sudden and steep decline in\\nperformance as the system approaches the limits of tolerance. This can be due\\nto input parameters which exceed a specified input, or environmental conditions\\nwhich exceed specified operating boundaries. An analogy is made between brittle\\ncommmunication systems in particular and materials science.', 'label': 'cs.*'}\n",
      "Sample test data: {'text': 'Sample-efficient machine learning (SEML) has been widely applied to find\\noptimal latency and power tradeoffs for configurable computer systems. Instead\\nof randomly sampling from the configuration space, SEML reduces the search cost\\nby dramatically reducing the number of configurations that must be sampled to\\noptimize system goals (e.g., low latency or energy). Nevertheless, SEML only\\nreduces one component of cost -- the total number of samples collected -- but\\ndoes not decrease the cost of collecting each sample. Critically, not all\\nsamples are equal; some take much longer to collect because they correspond to\\nslow system configurations. This paper present Cello, a computer systems\\noptimization framework that reduces sample collection costs -- especially those\\nthat come from the slowest configurations. The key insight is to predict ahead\\nof time whether samples will have poor system behavior (e.g., long latency or\\nhigh energy) and terminate these samples early before their measured system\\nbehavior surpasses the termination threshold, which we call it predictive early\\ntermination. To predict the future system behavior accurately before it\\nmanifests as high runtime or energy, Cello uses censored regression to produces\\naccurate predictions for running samples. We evaluate Cello by optimizing\\nlatency and energy for Apache Spark workloads. We give Cello a fixed amount of\\ntime to search a combined space of hardware and software configuration\\nparameters. Our evaluation shows that compared to the state-of-the-art SEML\\napproach in computer systems optimization, Cello improves latency by 1.19X for\\nminimizing latency under a power constraint, and improves energy by 1.18X for\\nminimizing energy under a latency constraint.', 'label': 'cs.*'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103cf58324eb451bb1cff5abf2504865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee42eddcc5a450f98d5699cca208ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8923ccc01700401ab057ee533dd03811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5393437dd742b589ae8243c8bc630c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360be7cd9ecd484396c1320ecf87db4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/30.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Voice49/arXiv-Abstract-Label-20k/commit/fb4323d849863602f6c50e0d631b4bce9e381d08', commit_message='Upload dataset', commit_description='', oid='fb4323d849863602f6c50e0d631b4bce9e381d08', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "loaded_dataset = load_from_disk(\"updated_arxiv_dataset\")\n",
    "\n",
    "\n",
    "loaded_dataset[\"train\"] = loaded_dataset[\"train\"].rename_column(\"abstract\", \"text\").rename_column(\"category\", \"label\")\n",
    "loaded_dataset[\"test\"] = loaded_dataset[\"test\"].rename_column(\"abstract\", \"text\").rename_column(\"category\", \"label\")\n",
    "print(loaded_dataset)\n",
    "\n",
    "# Print some sample data to verify\n",
    "print(\"Sample train data:\", loaded_dataset['train'][0])\n",
    "print(\"Sample test data:\", loaded_dataset['test'][0])\n",
    "\n",
    "loaded_dataset.push_to_hub(\"Voice49/arXiv-Abstract-Label-20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ef9448ffec415cbb9be6c15f4147c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dffaf0ee99430780d4e7b6ae1599cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6ef2f88156443593f0e2112f18db9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db758b6771748aa9f307347b12c894a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Voice49/arXiv-Abstract-primaryCategory/commit/1a69aa0412428eb464d35f15e7f2e30414e2f18e', commit_message='Upload dataset', commit_description='', oid='1a69aa0412428eb464d35f15e7f2e30414e2f18e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_dataset.push_to_hub(\"Voice49/arXiv-Abstract-primaryCategory\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
