{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model with Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset_info(dataset):\n",
    "    info = dataset.info\n",
    "    dataset_name = info.dataset_name\n",
    "    splits_info = info.splits\n",
    "    features = info.features\n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(\"Splits Info:\")\n",
    "    for split_name, split_info in splits_info.items():\n",
    "        num_examples = split_info.num_examples\n",
    "        print(f\" - Split: {split_name}, Num Examples: {num_examples}\")\n",
    "    print(\"Features:\")\n",
    "    for feature_name, feature_info in features.items():\n",
    "        print(f\" - {feature_name}: {feature_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def get_dataset(dataset_name, train_size=0, test_size=0):  \n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    #display_dataset_info(dataset['test'])\n",
    "    \n",
    "    # Access the train, test splits\n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "\n",
    "    # Random sample the dataset, only use random_sample_size\n",
    "    if(train_size != 0):\n",
    "        train_dataset = train_dataset.shuffle(seed=42).select(range(train_size))\n",
    "    if(test_size != 0):\n",
    "        test_dataset = test_dataset.shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import numpy as np\n",
    "\n",
    "def encode_Pipeline(model, dataset, key=\"text\", truncation=True, padding=True, max_length=512, use_cls=True):\n",
    "    data = KeyDataset(dataset, key)\n",
    "    pipe = model(data, return_tensors=True, truncation=truncation, padding=padding, max_length=max_length)\n",
    "    embeddings=[]\n",
    "    for tensor in tqdm(pipe, desc=\"Encoding\"): \n",
    "        # Tensor Shape [batch_size, sequence_length, hidden_size]\n",
    "        if use_cls:\n",
    "            embedding = tensor[:, 0, :]\n",
    "        else:\n",
    "            embedding = tensor.mean(dim=1)\n",
    "        embeddings.append(embedding.squeeze())\n",
    "    return np.array(embeddings), np.array(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructor - Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ST(model, dataset):\n",
    "    embeddings = []\n",
    "    texts = dataset[\"text\"]\n",
    "    instructions = dataset[\"instruction\"]\n",
    "    for text, instruction in tqdm(zip(texts, instructions),total=len(dataset), desc=\"Encoding\"):\n",
    "        embedding = model.encode([[instruction, text]])[0]\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings), np.array(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 - Transformer Sentence Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def encode_T5(dataset, model, key=\"text\", truncation=True, padding=True, max_length=512, use_mean_pooling=True):\n",
    "    # Check if CUDA is available and set device accordingly\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize the tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model)\n",
    "    model = T5Model.from_pretrained(model).to(device)\n",
    "    \n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    for data in tqdm(dataset, desc=\"Encoding text\"):\n",
    "        text = data[key]\n",
    "        label = data.get(\"label\", None)\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, truncation=truncation, padding=padding, max_length=max_length, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get encoder outputs\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = model.encoder(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        \n",
    "        # Pooling to get a single vector for each input\n",
    "        if use_mean_pooling:\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            last_hidden_state = encoder_outputs.last_hidden_state\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "            embedding = sum_embeddings / sum_mask\n",
    "        else:\n",
    "            embedding = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        embeddings.append(embedding.cpu().numpy().flatten())  # Flatten the embeddings\n",
    "        if label is not None:\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(embeddings), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_affixes(example, prefix, suffix):\n",
    "    example['text'] = prefix + example['text'] + suffix\n",
    "    return example\n",
    "\n",
    "def mapper_instruct(example, instruction):\n",
    "    example['instruction'] = instruction\n",
    "    return example\n",
    "\n",
    "def augment_dataset_Affix(dataset, prefix, suffix):\n",
    "    augmented_dataset = dataset.map(lambda x: mapper_affixes(x, prefix, suffix))\n",
    "    return augmented_dataset\n",
    "\n",
    "def augment_dataset_Inst(dataset, instruction):\n",
    "    augmented_dataset = dataset.map(lambda x: mapper_instruct(x, instruction))\n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(method, train_embeddings, test_embeddings, train_labels, test_labels):\n",
    "    if method == \"SVM\":\n",
    "        model = SVC(kernel='linear')\n",
    "        \n",
    "    elif method == \"MLP\":\n",
    "        model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4,\n",
    "                          solver='sgd', verbose=1, random_state=1,\n",
    "                          learning_rate_init=.1)\n",
    "\n",
    "    model.fit(train_embeddings, train_labels)\n",
    "    predicted_labels = model.predict(test_embeddings)\n",
    "    print(\"Report on \" + method + \": \")\n",
    "    print(classification_report(y_true = test_labels, y_pred = predicted_labels, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbedFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbedFlow_Bert_1(dataset_name, train_size, test_size, evaluator, prefix, suffix):\n",
    "    # Load Dataset\n",
    "    train_dataset, test_dataset = get_dataset(dataset_name, train_size, test_size)\n",
    "\n",
    "    # Load Model\n",
    "    model = pipeline(\"feature-extraction\", model=\"google-bert/bert-base-uncased\", device=0)\n",
    "    \n",
    "    train_dataset = augment_dataset_Affix(train_dataset, prefix, suffix)\n",
    "    test_dataset = augment_dataset_Affix(test_dataset, prefix, suffix)\n",
    "\n",
    "    # Embed Dataset\n",
    "    train_embeddings, train_labels = encode_Pipeline(model, train_dataset, use_cls=True)\n",
    "    test_embeddings, test_labels = encode_Pipeline(model, test_dataset, use_cls=True)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate(evaluator, train_embeddings, test_embeddings, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbedFlow_Bert_2(dataset_name, train_size, test_size, evaluator, prefix, suffix):\n",
    "    # Load Dataset\n",
    "    train_dataset, test_dataset = get_dataset(dataset_name, train_size, test_size)\n",
    "\n",
    "    # Load Model\n",
    "    model = pipeline(\"feature-extraction\", model=\"google-bert/bert-large-uncased\", device=0)\n",
    "    \n",
    "    train_dataset = augment_dataset_Affix(train_dataset, prefix, suffix)\n",
    "    test_dataset = augment_dataset_Affix(test_dataset, prefix, suffix)\n",
    "\n",
    "    # Embed Dataset\n",
    "    train_embeddings, train_labels = encode_Pipeline(model, train_dataset, use_cls=False)\n",
    "    test_embeddings, test_labels = encode_Pipeline(model, test_dataset, use_cls=False)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate(evaluator, train_embeddings, test_embeddings, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbedFlow_Instructor(dataset_name, train_size, test_size, evaluator, instruction):\n",
    "    # Load Dataset\n",
    "    train_dataset, test_dataset = get_dataset(dataset_name, train_size, test_size)\n",
    "\n",
    "    # Load Model\n",
    "    model = SentenceTransformer(\"hkunlp/instructor-large\")\n",
    "    \n",
    "    # Add Instruction\n",
    "    train_dataset = augment_dataset_Inst(train_dataset, instruction)\n",
    "    test_dataset = augment_dataset_Inst(test_dataset, instruction)\n",
    "\n",
    "    # Embed Dataset\n",
    "    train_embeddings, train_labels = encode_ST(model, train_dataset)\n",
    "    test_embeddings, test_labels = encode_ST(model, test_dataset)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate(evaluator, train_embeddings, test_embeddings, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbedFlow_T5(dataset_name, train_size, test_size, evaluator, prefix, suffix):\n",
    "    # Load Dataset\n",
    "    train_dataset, test_dataset = get_dataset(dataset_name, train_size, test_size)\n",
    "  \n",
    "    # Add Instruction\n",
    "    instruction = prefix  # Use prefix as the instruction\n",
    "    train_dataset = augment_dataset_Affix(train_dataset, prefix, suffix)\n",
    "    test_dataset = augment_dataset_Affix(test_dataset, prefix, suffix)\n",
    "\n",
    "    # Embed Dataset\n",
    "    train_embeddings, train_labels = en(train_dataset, model=\"t5-base\")\n",
    "    test_embeddings, test_labels = encode_T5(test_dataset, model=\"t5-base\")\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate(evaluator, train_embeddings, test_embeddings, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbedFlow_GPT(dataset_name, train_size, test_size, evaluator, prefix, suffix):\n",
    "    # Load Dataset\n",
    "    train_dataset, test_dataset = get_dataset(dataset_name, train_size, test_size)\n",
    "\n",
    "    # Load Model\n",
    "    model = pipeline(\"feature-extraction\", model=\"openai-community/gpt2\", device=0)\n",
    "  \n",
    "    # Add Instruction\n",
    "    train_dataset = augment_dataset_Affix(train_dataset, prefix, suffix)\n",
    "    test_dataset = augment_dataset_Affix(test_dataset, prefix, suffix)\n",
    "\n",
    "    # Embed Dataset\n",
    "    train_embeddings, train_labels = encode_Pipeline(model, train_dataset, use_cls=False)\n",
    "    test_embeddings, test_labels = encode_Pipeline(model, test_dataset, use_cls=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate(evaluator, train_embeddings, test_embeddings, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "[Google Sheet](https://docs.google.com/spreadsheets/d/1iBDq7C59G6olf_of_sTF5oCY3Itj6_kImzeUl3XMpd8/edit#gid=1587051763)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding:  12%|█▏        | 115/1000 [00:02<00:16, 54.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     25\u001b[0m test_size  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mEmbedFlow_GPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#EmbedFlow_GPT(datasets[0], train_size, test_size, evaluator[0], prefix[0], suffix[0])\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# for dataset in datasets:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#             # EmbedFlow_Bert_1(dataset, train_size, test_size,evaluator[0], instruction, '')\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#             EmbedFlow_GPT(dataset, test_size, test_size, evaluator[0], instruction)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[138], line 13\u001b[0m, in \u001b[0;36mEmbedFlow_GPT\u001b[1;34m(dataset_name, train_size, test_size, evaluator, prefix, suffix)\u001b[0m\n\u001b[0;32m     10\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m augment_dataset_Affix(test_dataset, prefix, suffix)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Embed Dataset\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m train_embeddings, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mencode_Bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m test_embeddings, test_labels \u001b[38;5;241m=\u001b[39m encode_Bert(model, test_dataset, use_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[129], line 9\u001b[0m, in \u001b[0;36mencode_Bert\u001b[1;34m(model, dataset, key, truncation, padding, max_length, use_cls)\u001b[0m\n\u001b[0;32m      7\u001b[0m pipe \u001b[38;5;241m=\u001b[39m model(data, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39mtruncation, padding\u001b[38;5;241m=\u001b[39mpadding, max_length\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[0;32m      8\u001b[0m embeddings\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEncoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Tensor Shape [batch_size, sequence_length, hidden_size]\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cls\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Voice\\anaconda3\\envs\\embed\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Voice\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Voice\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Voice\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformers\\pipelines\\base.py:1151\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1149\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m-> 1151\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFramework \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Voice\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformers\\pipelines\\base.py:1051\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[1;34m(self, inputs, device)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_tensor_on_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, device):\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, ModelOutput):\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutput(\n\u001b[1;32m-> 1051\u001b[0m             {name: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1052\u001b[0m         )\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\Voice\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformers\\pipelines\\base.py:1060\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[1;34m(self, inputs, device)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(item, device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs])\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Voice\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformers\\pipelines\\base.py:1060\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[1;34m(self, inputs, device)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(item, device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs])\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Voice\\anaconda3\\envs\\embed\\Lib\\site-packages\\transformers\\pipelines\\base.py:1062\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[1;34m(self, inputs, device)\u001b[0m\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(item, device) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m inputs])\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "datasets = ['stanfordnlp/imdb', \n",
    "            'yelp_review_full',\n",
    "            'Voice49/arXiv-Abstract-Label-20k']\n",
    "\n",
    "evaluator = ['SVM', 'MLP']\n",
    "\n",
    "prefix = ['',\n",
    "          'Movie Review: ', \n",
    "          'Restaurant Review: ', \n",
    "          'Sentiment Analysis: ', \n",
    "          'User Feedback: ', \n",
    "          'Customer Experience: ',\n",
    "          'Product Review: ',\n",
    "          'Service Feedback: ',\n",
    "          'Experience at: ',\n",
    "          'Abstract: ',\n",
    "          'Research Paper Abstract:',\n",
    "          'Paper Summary: ']\n",
    "suffix = ['','']\n",
    "\n",
    "train_size = 1000\n",
    "test_size  = 1000\n",
    "EmbedFlow_GPT(datasets[0], train_size, test_size, evaluator[0], prefix[1], '')\n",
    "#EmbedFlow_GPT(datasets[0], train_size, test_size, evaluator[0], prefix[0], suffix[0])\n",
    "\n",
    "# for dataset in datasets:\n",
    "#         for instruction in prefix:\n",
    "#             print(f\"Processing dataset: {dataset}, instruction: '{instruction}'\")\n",
    "#             # EmbedFlow_Bert_1(dataset, train_size, test_size,evaluator[0], instruction, '')\n",
    "#             EmbedFlow_GPT(dataset, test_size, test_size, evaluator[0], instruction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
