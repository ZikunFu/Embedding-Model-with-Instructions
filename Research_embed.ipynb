{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model with Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset_info(dataset):\n",
    "    info = dataset.info\n",
    "    dataset_name = info.dataset_name\n",
    "    splits_info = info.splits\n",
    "    features = info.features\n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(\"Splits Info:\")\n",
    "    for split_name, split_info in splits_info.items():\n",
    "        num_examples = split_info.num_examples\n",
    "        print(f\" - Split: {split_name}, Num Examples: {num_examples}\")\n",
    "    print(\"Features:\")\n",
    "    for feature_name, feature_info in features.items():\n",
    "        print(f\" - {feature_name}: {feature_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def get_dataset(dataset_name, train_size=0, test_size=0):  \n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    #display_dataset_info(dataset['test'])\n",
    "    \n",
    "    # Access the train, test splits\n",
    "    train_dataset = dataset['train']\n",
    "    test_dataset = dataset['test']\n",
    "\n",
    "    # Random sample the dataset, only use random_sample_size\n",
    "    if(train_size != 0):\n",
    "        train_dataset = train_dataset.shuffle(seed=42).select(range(train_size))\n",
    "    if(test_size != 0):\n",
    "        test_dataset = test_dataset.shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "\n",
    "def load_model(model_name):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    match model_name:\n",
    "        case \"Bert1\":\n",
    "            return pipeline(\"feature-extraction\", model=\"google-bert/bert-base-uncased\", device=0)\n",
    "        case \"Bert2\":\n",
    "            return pipeline(\"feature-extraction\", model=\"google-bert/bert-large-uncased\", device=0)\n",
    "        case \"Instructor\":\n",
    "            return SentenceTransformer(\"hkunlp/instructor-large\")\n",
    "        case _:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import numpy as np\n",
    "\n",
    "def encode_Bert(model, dataset, key=\"text\", truncation=True, padding=True, max_length=512, use_cls=True):\n",
    "    data = KeyDataset(dataset, key)\n",
    "    pipe = model(data, return_tensors=True, truncation=truncation, padding=padding, max_length=max_length)\n",
    "    embeddings=[]\n",
    "    for tensor in tqdm(pipe, desc=\"Encoding\"): \n",
    "        if use_cls:\n",
    "            # Shape [batch_size, sequence_length, hidden_size]\n",
    "            embedding = tensor[0, 0, :].detach().numpy()\n",
    "            print(embedding.shape)\n",
    "        else:\n",
    "            embedding = tensor.mean(dim=1).flatten()\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings), np.array(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructor - Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ST(model, dataset):\n",
    "    embeddings = []\n",
    "    texts = dataset[\"text\"]\n",
    "    instructions = dataset[\"instruction\"]\n",
    "    for text, instruction in tqdm(zip(texts, instructions),total=len(dataset), desc=\"Encoding\"):\n",
    "        embedding = model.encode([[instruction, text]])[0]\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings), np.array(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_affixes(example, prefix, suffix):\n",
    "    example['text'] = prefix + example['text'] + suffix\n",
    "    return example\n",
    "\n",
    "def mapper_instruct(example, instruction):\n",
    "    example['instruction'] = instruction\n",
    "    return example\n",
    "\n",
    "def augment_dataset_Affix(dataset, prefix, suffix):\n",
    "    augmented_dataset = dataset.map(lambda x: mapper_affixes(x, prefix, suffix))\n",
    "    return augmented_dataset\n",
    "\n",
    "def augment_dataset_Inst(dataset, instruction):\n",
    "    augmented_dataset = dataset.map(lambda x: mapper_instruct(x, instruction))\n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(method, train_embeddings, test_embeddings, train_labels, test_labels):\n",
    "    if method == \"SVM\":\n",
    "        model = SVC(kernel='linear')\n",
    "        \n",
    "    elif method == \"MLP\":\n",
    "        model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4,\n",
    "                          solver='sgd', verbose=1, random_state=1,\n",
    "                          learning_rate_init=.1)\n",
    "\n",
    "    model.fit(train_embeddings, train_labels)\n",
    "    predicted_labels = model.predict(test_embeddings)\n",
    "    print(\"Report on \" + method + \": \")\n",
    "    print(classification_report(y_true = test_labels, y_pred = predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbedFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbedFlow_Bert_1(dataset_name, train_size, test_size, evaluator, prefix, suffix):\n",
    "    # Load Dataset\n",
    "    train_dataset, test_dataset = get_dataset(dataset_name, train_size, test_size)\n",
    "\n",
    "    # Load Model\n",
    "    model = pipeline(\"feature-extraction\", model=\"google-bert/bert-base-uncased\", device=0)\n",
    "    \n",
    "    train_dataset = augment_dataset_Affix(train_dataset, prefix, suffix)\n",
    "    test_dataset = augment_dataset_Affix(test_dataset, prefix, suffix)\n",
    "\n",
    "    # Embed Dataset\n",
    "    train_embeddings, train_labels = encode_Bert(model, train_dataset, use_cls=False)\n",
    "    test_embeddings, test_labels = encode_Bert(model, test_dataset, use_cls=False)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate(evaluator, train_embeddings, test_embeddings, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbedFlow_Bert_2(dataset_name, train_size, test_size, evaluator, prefix, suffix):\n",
    "    # Load Dataset\n",
    "    train_dataset, test_dataset = get_dataset(dataset_name, train_size, test_size)\n",
    "\n",
    "    # Load Model\n",
    "    model = pipeline(\"feature-extraction\", model=\"google-bert/bert-large-uncased\", device=0)\n",
    "    \n",
    "    train_dataset = augment_dataset_Affix(train_dataset, prefix, suffix)\n",
    "    test_dataset = augment_dataset_Affix(test_dataset, prefix, suffix)\n",
    "\n",
    "    # Embed Dataset\n",
    "    train_embeddings, train_labels = encode_Bert(model, train_dataset, use_cls=False)\n",
    "    test_embeddings, test_labels = encode_Bert(model, test_dataset, use_cls=False)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate(evaluator, train_embeddings, test_embeddings, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbedFlow_Instructor(dataset_name, train_size, test_size, evaluator, prefix, suffix):\n",
    "    # Load Dataset\n",
    "    train_dataset, test_dataset = get_dataset(dataset_name, train_size, test_size)\n",
    "\n",
    "    # Load Model\n",
    "    model = SentenceTransformer(\"hkunlp/instructor-large\")\n",
    "    \n",
    "    # Add Instruction\n",
    "    instruction = prefix  # Use prefix as the instruction\n",
    "    train_dataset = augment_dataset_Inst(train_dataset, instruction)\n",
    "    test_dataset = augment_dataset_Inst(test_dataset, instruction)\n",
    "\n",
    "    # Embed Dataset\n",
    "    train_embeddings, train_labels = encode_ST(model, train_dataset)\n",
    "    test_embeddings, test_labels = encode_ST(model, test_dataset)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate(evaluator, train_embeddings, test_embeddings, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "[Google Sheet](https://docs.google.com/spreadsheets/d/1iBDq7C59G6olf_of_sTF5oCY3Itj6_kImzeUl3XMpd8/edit#gid=1587051763)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfdac6e554a42dfb4b8e0da41350760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36148bface994e0abba0db294ac0dde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edd7ba739e94e94bb5e69d044872e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a63b0329b24a348ed8c961783880d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report on SVM: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cs.*       0.86      0.85      0.85      1250\n",
      "      econ.*       0.66      0.34      0.45      1250\n",
      "      eess.*       0.91      0.88      0.89      1250\n",
      "      math.*       0.97      0.96      0.96      1250\n",
      "   physics.*       0.86      0.88      0.87      1250\n",
      "     q-bio.*       0.90      0.88      0.89      1250\n",
      "     q-fin.*       0.86      0.85      0.85      1250\n",
      "      stat.*       0.55      0.85      0.67      1250\n",
      "\n",
      "    accuracy                           0.81     10000\n",
      "   macro avg       0.82      0.81      0.81     10000\n",
      "weighted avg       0.82      0.81      0.81     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = ['stanfordnlp/imdb', \n",
    "            'yelp_review_full',\n",
    "            'Voice49/arXiv-Abstract-Label-20k']\n",
    "\n",
    "evaluator = ['SVM', 'MLP']\n",
    "\n",
    "prefix = ['','Movie Review: ', 'Cat and Dog: ', \n",
    "          'Restaurant Review: ', \n",
    "          'Sentiment Analysis: ', \n",
    "          'User Feedback: ', \n",
    "          'Customer Experience: ',\n",
    "          'Product Review: ',\n",
    "          'Service Feedback:',\n",
    "          'Experience at: ']\n",
    "suffix = ['','']\n",
    "\n",
    "train_size = 0\n",
    "test_size  = 0\n",
    "\n",
    "EmbedFlow_Bert_1(datasets[2], train_size, test_size, evaluator[0], prefix[0], suffix[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
